---
title: "Supervised Learning"
author: "<you_names_here>"
date: "06/01/2025"
output: html_document
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("jsonlite", warn.conflicts = FALSE)
library("ggplot2", warn.conflicts = FALSE)
library("lattice", warn.conflicts = FALSE)
library("caret", warn.conflicts = FALSE)
library("gbm", warn.conflicts = FALSE)
library("pROC", warn.conflicts = FALSE)

library("e1071", warn.conflicts = FALSE)
library("ModelMetrics", warn.conflicts = FALSE)


set.seed(42)
```

# Detección de ataques con aprendizaje supervisado

El siguiente ejercicio consiste en la optmización de un modelo de Machine Learning capaz de detectar ataques a partir de logs de un firewall. Para este propósito, se realizará una prueba de concepto con una pequeña muestra de logs previamente etiquetados como tráfico normal o ataque.

## Load of the data sets

Se proporcionan los siguentes archivos:

-   features.csv
-   events.csv

```{r tidy_data, echo=FALSE}

base_path <- "./"
# por ejemplo....
#base_path <- "https://github.com/mirnager/AE4_DDS/tree/development/fernando/Modelo_Inicial"

events <- read.csv(paste(base_path, "events_sample.csv", sep = ""))
features <- read.csv(paste(base_path, "features.csv", sep = ""))
```

### Events analysis/exploration


```{r events_stats, echo=FALSE}


```

### Data enrichment

```{r data_enrich, echo=FALSE}


```

## Feature engineering

```{r feat_eng, echo=FALSE}
# El modelo requiere nombres de columna simples y features numericas o factor
names(events) <- stringr::str_replace_all(names(events), "_", "")
events <- as.data.frame(unclass(events), stringsAsFactors = TRUE)

# Etiquetamos la columna Label con valores categoricos
events$Label <- ifelse(events$Label == 1, "ATTACK", "NORMAL")
events$Label <- as.factor(events$Label)
events$attackcat <- NULL

outcomeName <- 'Label'
predictorsNames <- names(events)[names(events) != outcomeName]

prop.table(table(events$Label))


```

```{r prueba, include=FALSE}

#PRUEBAS######_____

tabla_Label_Attack <- subset(events, Label == "ATTACK")

tabla_DsPort_Attack <- table(tabla_Label_Attack$dsport)
print(tabla_DsPort_Attack)

tabla_DsIP_Attack <- table(tabla_Label_Attack$dstip)
print(tabla_DsIP_Attack)

```


## Build model

### Create train and test data sets

```{r train_test, echo=FALSE}
splitIndex <- caret::createDataPartition(events[,outcomeName], p = .70, list = FALSE, times = 1)

trainDF <- events[ splitIndex,]
testDF  <- events[-splitIndex,]


# createDataPartition() mantiene la proporción original de la variable objetivo (outcomeName) en ambos conjuntos.
#Ejemplo:
#Si tienes 1000 datos, donde:
# 700 son "NORMAL" (70%)
# 300 son "ATTACK" (30%)
# Con p = 0.80, el conjunto de entrenamiento mantendría aproximadamente 560 "NORMAL" y 240 "ATTACK" (70%-30%), en lugar de una distribución aleatoria que podría desbalancear el modelo.

# Conjunto de Datos - A continuacion se muestran la consideración para clasificar los datos en pequeños, medianos, grandes muy grandes y asi poder determinar p:
#Pequeño (≤ 1,000) → Riesgo de sobreajuste, usar validación cruzada.
#Mediano (1,000 – 100,000) → Balance entre entrenamiento y prueba.
#Grande (≥ 100,000) → Se requieren optimizaciones computacionales.
#Muy Grande (≥ 10,000,000) → Técnicas de Big Data y Deep Learning.

```

### Prepare object with training configuration (how we are gonna train the model)

```{r model_config, echo=FALSE}
# Consulta https://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada
objControl <- caret::trainControl(method = 'none',
                           returnResamp = 'none',
                           summaryFunction = twoClassSummary,
                           classProbs = TRUE)

```

```{r Código Mejorado Validación Cruzada, echo=FALSE}

#En el código actual, el modelo se entrena sin validación cruzada (method = 'none'). Esto significa que el modelo se entrena con un único conjunto de datos de entrenamiento, lo que puede llevar a sobreajuste (overfitting) y a una menor generalización del modelo.
#Cambios propuestos:
#-Implementar validación cruzada para evaluar el modelo en múltiples subconjuntos de datos.
#-Usar un método de validación cruzada como cv (cross-validation) con un número adecuado de folds (por ejemplo, 5 o 10).

objControl2 <- caret::trainControl(
  method = 'cv',          # Usar validación cruzada
  number = 5,             # Número de folds (por ejemplo, 5)
  returnResamp = 'none',
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = TRUE  # Guardar predicciones para evaluación
)


```



### Train the model

```{r model_train, echo=FALSE}
objModel <- caret::train(trainDF[,predictorsNames], trainDF[,outcomeName],
                  method = 'gbm',
                  trControl = objControl,
                  metric = "ROC",
                  preProc = c("center", "scale"))
# summary(objModel)



```

```{r Codigo Mejorado Grid Search, echo=FALSE}

# Definir la cuadrícula de hiperparámetros
gbmGrid <- expand.grid(
  n.trees = c(300),       # Número de árboles
  interaction.depth = c(3, 5, 7),  # Profundidad de los árboles
  shrinkage = c(0.01, 0.1),        # Tasa de aprendizaje
  n.minobsinnode = 10              # Mínimo de observaciones en nodos terminales
)

# Entrenar el modelo con la cuadrícula de hiperparámetros
objModel2 <- caret::train(
  trainDF[, predictorsNames], trainDF[, outcomeName],
  method = 'gbm',
  trControl = objControl2,
  metric = "ROC",
  preProc = c("center", "scale"),
  tuneGrid = gbmGrid  # Usar la cuadrícula de hiperparámetros
)

Valores_Guardados_Prediccion <- objModel2$pred

results <- objModel2$results  # Datos del modelo ajustado
plot(results$n.trees, results$ROC, type = 'b', pch = 19, col = 'blue',
     xlab = 'Número de árboles', ylab = 'AUC', main = 'AUC vs. Número de árboles')


```


### Test model

```{r model_test, echo=FALSE}
predictions <- predict(object = objModel, testDF[, predictorsNames], type = 'raw')
#head(predictions)
```

```{r Codigo Mejorado model_test, echo=FALSE}

predictions2 <- predict(object = objModel2, testDF[, predictorsNames], type = 'raw')
#head(predictions)

```



## Evaluate model

```{r model_eval, echo=FALSE}
print(caret::postResample(pred = predictions, obs = as.factor(testDF[,outcomeName])))
```

```{r Codigo Mejorado model_eval, echo=FALSE}
print(caret::postResample(pred = predictions2, obs = as.factor(testDF[,outcomeName])))
```


```{r predic_prob}
# probabilites
predictions <- predict(object = objModel, testDF[,predictorsNames], type = 'prob')
auc <- pROC::roc(ifelse(testDF[,outcomeName] == "ATTACK",1,0), predictions[[2]])
print(auc$auc)
```


```{r Codigo mejorado predic_prob}
# probabilites
predictions2 <- predict(object = objModel2, testDF[,predictorsNames], type = 'prob')
auc <- pROC::roc(ifelse(testDF[,outcomeName] == "ATTACK",1,0), predictions2[[2]])
print(auc$auc)



#auc$auc:
#Este comando extrae el valor de AUC de la curva ROC calculada en el paso anterior. El valor del AUC es un número entre 0 y 1:
#AUC ≈ 1: El modelo tiene una excelente capacidad de clasificación.
#AUC ≈ 0.5: El modelo no es mejor que una clasificación aleatoria.
#AUC ≈ 0: El modelo está completamente equivocado en sus predicciones.

#Resumen del código:
#Predicción de probabilidades: El modelo realiza predicciones en forma de probabilidades para cada clase, no solo las clases directas.
#Cálculo de la curva ROC y AUC: Se calcula la curva ROC comparando las probabilidades de la clase positiva ("ATTACK") con las etiquetas reales del conjunto de prueba.
#Evaluación del modelo: El valor del AUC nos da una medida de cuán bien el modelo puede discriminar entre las clases. Un AUC más alto indica un mejor rendimiento en términos de clasificación.
#basicamente lo que se hace es comparar los valores del TestDF con el resultado (en valores de probabilidad) de los valores entrenados, por defecto si la probabilidad pasa del 0,5 se considera positivo(por desirlo de alguna manera seria igual a 1) e inferior se considera negativo


```

```{r var_importance}
plot(caret::varImp(objModel, scale = F))



```



```{r Codigo Mejorado var_importance}
plot(caret::varImp(objModel2, scale = F))



```

## Conclusiones

Aqui deben incluirse los cambios hechos en el codigo que han incrementado la precision del modelo.
```{r conclusion, echo=FALSE}
# opcional, solo rellenar si hay algo de codigo que mostrar. Borrar en caso contrario

```
